{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMQ5mT6D31Dw51P743CSvR7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Engbasemhamada/Hotel-Booking-EDA/blob/main/task2_of_hotel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNbkXncLwyBs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.impute import SimpleImputer\n",
        "import seaborn as sns\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "# Creating a manual dataset\n",
        "data = pd.DataFrame({\n",
        "    'name': ['John', 'Jane', 'Jack', 'John', None],\n",
        "    'age': [28, 34, None, 28, 22],\n",
        "    'purchase_amount': [100.5, None, 85.3, 100.5, 50.0],\n",
        "    'date_of_purchase': ['2023/12/01', '2023/12/02', '2023/12/01', '2023/12/01', '2023/12/03']\n",
        "\t});\n",
        "\n",
        "# Handling missing values using mean imputation for 'age' and 'purchase_amount'\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data[['age', 'purchase_amount']] = imputer.fit_transform(data[['age', 'purchase_amount']])\n",
        "\n",
        "# Removing duplicate rows\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# Correcting inconsistent date formats\n",
        "data['date_of_purchase'] = pd.to_datetime(data['date_of_purchase'], errors='coerce')\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating two manual datasets\n",
        "data1 = pd.DataFrame({\n",
        "    'customer_id': [1, 2, 3],\n",
        "    'name': ['John', 'Jane', 'Jack'],\n",
        "    'age': [28, 34, 29]\n",
        "})\n",
        "\n",
        "data2 = pd.DataFrame({\n",
        "    'customer_id': [1, 3, 4],\n",
        "    'purchase_amount': [100.5, 85.3, 45.0],\n",
        "    'purchase_date': ['2023-12-01', '2023-12-02', '2023-12-03']\n",
        "})\n",
        "\n",
        "# Merging datasets on a common key 'customer_id'\n",
        "merged_data = pd.merge(data1, data2, on='customer_id', how='inner')\n",
        "\n",
        "print(merged_data)"
      ],
      "metadata": {
        "id": "tDsbwf4Nk4Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_x,diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
        "diabetes_x = diabetes_x[:,np.newaxis,2]\n",
        "diabetes_x_train = diabetes_x[:-20]\n",
        "diabetes_x_test = diabetes_x[-20:]\n",
        "\n",
        "diabetes_y_train = diabetes_y[:-20]\n",
        "diabetes_y_test = diabetes_y[-20:]"
      ],
      "metadata": {
        "id": "RpiEqb_3xKDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code for one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=['categorical_column'])\n",
        "# Example code for label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['categorical_column'] = le.fit_transform(df['categorical_column'])"
      ],
      "metadata": {
        "id": "2nMdvkYk6owr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a manual dataset\n",
        " data = pd.DataFrame({\n",
        "    'name': ['John', 'Jane', 'Jack', 'John', None],\n",
        "    'age': [28, 34, None, 28, 22],\n",
        "    'purchase_amount': [100.5, None, 85.3, 100.5, 50.0],\n",
        "    'date_of_purchase': ['2023/12/01', '2023/12/02', '2023/12/01', '2023/12/01', '2023/12/03']\n",
        "\t})"
      ],
      "metadata": {
        "id": "mZxgKxkj6pjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a manual dataset\n",
        "data = pd.DataFrame({\n",
        "    'name': ['John', 'Jane', 'Jack', 'John', None],\n",
        "    'age': [28, 34, None, 28, 22],\n",
        "    'purchase_amount': [100.5, None, 85.3, 100.5, 50.0],\n",
        "    'date_of_purchase': ['2023/12/01', '2023/12/02', '2023/12/01', '2023/12/01', '2023/12/03']\n",
        "\t})\n",
        "\n",
        "# Handling missing values using mean imputation for 'age' and 'purchase_amount'\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data[['age', 'purchase_amount']] = imputer.fit_transform(data[['age', 'purchase_amount']])\n",
        "\n",
        "# Removing duplicate rows\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# Correcting inconsistent date formats\n",
        "data['date_of_purchase'] = pd.to_datetime(data['date_of_purchase'], errors='coerce')\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "Xl_GWiYLViga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Creating a manual dataset\n",
        "data = pd.DataFrame({\n",
        "    'category': ['A', 'B', 'A', 'C', 'B'],\n",
        "    'numeric_column': [10, 15, 10, 20, 15]\n",
        "\t})"
      ],
      "metadata": {
        "id": "AgKUe9dinsDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling numeric data\n",
        "scaler = StandardScaler()\n",
        "data['scaled_numeric_column'] = scaler.fit_transform(data[['numeric_column']])"
      ],
      "metadata": {
        "id": "pVF7h8zont1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical variables using one-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_data = pd.DataFrame(encoder.fit_transform(data[['category']]),\n",
        "                            columns=encoder.get_feature_names_out(['category']))\n",
        "data = pd.concat([data, encoded_data], axis=1)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "8nJjKfDcocL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Applying Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "data['category_encoded'] = label_encoder.fit_transform(data['category'])\n",
        "\n",
        "data = pd.concat([data, encoded_data], axis=1)\n",
        "\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "_cJE_6cgrBFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating the encoded data with the original dataset\n",
        "data = pd.concat([data, encoded_data], axis=1)"
      ],
      "metadata": {
        "id": "FMUxaNgCohz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "OLCJKG4eojvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Creating a manual dataset\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [10, 20, 30, 40, 50],\n",
        "    'feature2': [1, 2, 3, 4, 5],\n",
        "    'feature3': [100, 200, 300, 400, 500],\n",
        "    'target': [0, 1, 0, 1, 0]\n",
        "\t})\n",
        "\n",
        "# Feature selection using SelectKBest\n",
        "selector = SelectKBest(chi2, k=2)\n",
        "selected_features = selector.fit_transform(data[['feature1', 'feature2', 'feature3']], data['target'])\n",
        "\n",
        "# Printing selected features\n",
        "print(\"Selected features (SelectKBest):\")\n",
        "print(selected_features)\n",
        "\n",
        "# Dimensionality reduction using PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_data = pca.fit_transform(data[['feature1', 'feature2', 'feature3']])\n",
        "\n",
        "# Printing PCA results\n",
        "print(\"PCA reduced data:\")\n",
        "print(pca_data)"
      ],
      "metadata": {
        "id": "x-3Jhv3colw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample= [15, 101, 18, 7, 13, 16, 11, 21, 5, 15, 10, 9]\n",
        "plt.boxplot(sample, vert=False)\n",
        "plt.title(\"Detecting outliers using Boxplot\")\n",
        "plt.xlabel('Sample')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bOD46ovmJlHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 2. Load Dataset\n",
        "data = pd.read_csv(\"first inten project.csv\")  # ØºÙŠÙ‘Ø± Ø§Ù„Ø§Ø³Ù… Ø­Ø³Ø¨ Ø§Ù„Ù…Ù„Ù Ø¨ØªØ§Ø¹Ùƒ\n",
        "data"
      ],
      "metadata": {
        "id": "yDecX_KGYxjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Data Preprocessing\n",
        "print(\" Data Info:\")\n",
        "print(data.info())\n",
        "\n",
        "# Missing values\n",
        "print(\"\\n Missing Values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Strip spaces in column names\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Fill missing numeric values with median\n",
        "data = data.fillna(data.median(numeric_only=True))\n"
      ],
      "metadata": {
        "id": "EpNpKk0jf3jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Handle Outliers (IQR Method)\n",
        "def remove_outliers_iqr(df, col):\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
        "    return df\n",
        "\n",
        "num_cols = data.select_dtypes(include=np.number).columns\n",
        "for col in num_cols:\n",
        "    data = remove_outliers_iqr(data, col)\n",
        "data"
      ],
      "metadata": {
        "id": "COegzeOVr4p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(\"booking status\", axis=1)\n",
        "y = data[\"booking status\"]\n",
        "\n",
        "# Ù„Ùˆ Ø§Ù„Ù‡Ø¯Ù Ù†ØµÙŠ â†’ Ù†Ø­ÙˆÙ„Ù‡ Ù„Ø£Ø±Ù‚Ø§Ù…\n",
        "if y.dtype == \"object\":\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "\n",
        "# Ø§Ø®ØªÙŠØ§Ø±ÙŠ: Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø§Ù„Ù…ÙŠØ²Ø§Øª (Ù‡Ù†Ø§ 5 Ù…Ø¨Ø¯Ø¦ÙŠÙ‹Ø§)\n",
        "if len(num_cols) > 2:  # Ù†ØªØ£ÙƒØ¯ ÙÙŠÙ‡ Ø£Ø¹Ù…Ø¯Ø© ÙƒÙØ§ÙŠØ©\n",
        "    selector = SelectKBest(chi2, k=min(5, len(num_cols)))\n",
        "    X_selected = selector.fit_transform(X[num_cols], y)\n",
        "else:\n",
        "    X_selected = X[num_cols]\n",
        "\n",
        "# PCA Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
        "if X_selected.shape[1] > 2:\n",
        "    pca = PCA(n_components=2)\n",
        "    X_reduced = pca.fit_transform(X_selected)\n",
        "else:\n",
        "    X_reduced = X_selected\n"
      ],
      "metadata": {
        "id": "mmXSjxLPtFnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Transform Categorical Data\n",
        "# ==============================\n",
        "cat_cols = X.select_dtypes(include=\"object\").columns\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    X[col] = le.fit_transform(X[col])\n"
      ],
      "metadata": {
        "id": "n8MJc5bQv9sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Scaling\n",
        "# ==============================\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "1B5qXsnUx3pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train-Test Split\n",
        "# ==============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2fF6gpdhx8Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ 8. Modeling\n",
        "# ==============================\n",
        "# Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_lr = log_reg.predict(X_test)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "G3F1i4Khx_2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ 9. Evaluation\n",
        "# ==============================\n",
        "print(\"\\nðŸ“ˆ Logistic Regression\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "print(\"\\nðŸŒ³ Random Forest\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "6nJDluPsyLEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}